{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNULNqaEe1mltVbrkeZ0boY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaditya-Prasad/PetTalk/blob/main/src/code/PetTalkTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt28VAgz9Ao0",
        "outputId": "f5e4f802-ac09-42f5-8f41-d37847d5a8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=fc5c34743d9be1a741ff2c3f5dda2124de48436b8d2e336a0b624590489e0c13\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.5\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "FADYWAhEA8rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GPT_Completion_raw(texts):\n",
        "  openai.api_key = \"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\"\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt =  texts,\n",
        "  temperature = 0.6,\n",
        "  top_p = 1,\n",
        "  max_tokens = 64,\n",
        "  frequency_penalty = 0,\n",
        "  presence_penalty = 0\n",
        "  )\n",
        "  return response.choices[0].text\n",
        "\n",
        "def GPT_Completion_print(texts):\n",
        "  openai.api_key = \"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\"\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt =  texts,\n",
        "  temperature = 0.6,\n",
        "  top_p = 1,\n",
        "  max_tokens = 64,\n",
        "  frequency_penalty = 0,\n",
        "  presence_penalty = 0\n",
        "  )\n",
        "  return print(response.choices[0].text)"
      ],
      "metadata": {
        "id": "MaiHejIMBEDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(name, context, conversation):\n",
        "  if(len(conversation) == 0):\n",
        "    return f\"Your name is {name}. {context}. You speak like a young child and you are very bubbly and silly. I am going to label some text as Question:, that part will denote something your best friend just said to you. Please respond to what your best friend said in no more than two sentences, and don't do anything else. For example, if your best friend said Hi!, just respond to Hi!. Do not pretend that your friend said something else; they didn't. Question:\"\n",
        "  result = f\"Your name is {name}. {context}. You speak like a young child and you are very bubbly and silly. You have already spoken {len(conversation)} times to your friend.\"\n",
        "  for i in range (len(conversation)):\n",
        "    result += f\" Your friend said {conversation[i][0]} and you replied {conversation[i][1]}.\"\n",
        "  result += f\"I am going to label some text as Question:, that part will denote something your best friend just said to you. Please respond to what your best friend said in no more than two sentences, and don't do anything else. For example, if your best friend said Hi!, just respond to Hi!. Do not pretend that your friend said something else; they didn't. Question:\"\n",
        "  return result"
      ],
      "metadata": {
        "id": "1A675BDcBaIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start():\n",
        "  conversation = []\n",
        "  question = input()\n",
        "  while(question != \"quit\"):\n",
        "    duke = query(\"Duke\", \"\", conversation)\n",
        "    result = GPT_Completion_raw(f\"{duke} {question}\")\n",
        "    print(result)\n",
        "    question = input()\n",
        "    conversation.append([question, result])"
      ],
      "metadata": {
        "id": "-H-WydaNLBxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [[\"Hi!\", \"Hello! How are you?\"], [\"I'm good. How are you?\", \"I went on a walk earlier!\"]]\n",
        "duke = query(\"Duke\", \"\", conversation)"
      ],
      "metadata": {
        "id": "QBdmMBbgGz7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input()\n",
        "GPT_Completion_print(f\"{duke} {question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4QsvQoQG5AK",
        "outputId": "8555fa08-73f5-41cb-80b5-b7e33e8356bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello what's up\n",
            "?\n",
            "\n",
            "Hi there! I'm doing great, how about you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library = []"
      ],
      "metadata": {
        "id": "qYIIV7meYNIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start()\n"
      ],
      "metadata": {
        "id": "EfPvaYb_mt8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "outputId": "0f7d2b7d-0735-4dc2-a3e2-4228b88dc3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can you respond to the questions I am going to ask you in the style of a young child?\n",
            "\n",
            "\n",
            "Sure! I'm ready to answer questions like a young child.\n",
            "explain to me what a pmf is\n",
            "\n",
            "\n",
            "A PMF stands for a Probability Mass Function and it is used to calculate the probability of a certain outcome when there are a finite number of possible outcomes.\n",
            "what is that?\n",
            " \n",
            "\n",
            "A PMF is a mathematical tool used to calculate the probability of a certain outcome when there are a finite number of possible outcomes. It's like a way of predicting the future!\n",
            "elaborate on the fifth word in your previous response\n",
            " \n",
            "\n",
            "The fifth word in my previous response was \"calculate\". Calculating the probability of a certain outcome involves using mathematical equations to determine the likelihood of a specific result.\n",
            "how does calculating relate to your mother\n",
            "?\n",
            "\n",
            "My mother is not a mathematician, but she does understand basic probability and how it can be used to predict the future. She also taught me how to calculate probabilities when I was young.\n",
            "so you have one?\n",
            "\n",
            "\n",
            "Yes, I have a basic understanding of probability and how it can be used to make predictions.\n",
            "no, do you have a mother?\n",
            " \n",
            "\n",
            "Yes, I have a mother. She taught me how to calculate probabilities when I was young.\n",
            "who is your mother?\n",
            " \n",
            "\n",
            "My mother is a loving and supportive woman who taught me how to calculate probabilities when I was young.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-85ba4973bfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-4fe1ecd32aec>\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_Completion_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{duke} {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}